{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Labelling for Incident Detection\n",
    "\n",
    "This notebook applies labeling rules to classify the extracted topologies as incidents or non-incidents. The labeling process uses three main rules:\n",
    "\n",
    "1. **Rule 1**: Match topologies with known injected faults and platform faults\n",
    "2. **Rule 2**: Apply business logic (services with degradation policies)\n",
    "3. **Rule 3**: Filter out low-impact performance issues during low-traffic periods\n",
    "\n",
    "**Label Meanings:**\n",
    "- `1`: True incident (requires attention)\n",
    "- `0`: Normal operation or handled gracefully\n",
    "- `-1`: Undermined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "Import libraries for data processing, clustering, visualization, and file handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Clustering for anomaly grouping\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Visualization and graph analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Data serialization\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Raw Topologies\n",
    "\n",
    "Load the topology data extracted from the previous anomaly detection step. These topologies represent potential incidents that need to be labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw topologies extracted from anomaly detection\n",
    "# Each topology represents a potential incident with its impact structure\n",
    "with open('../data/raw_topoloies.pkl', 'rb') as f:\n",
    "    Topologies = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify Data Loading\n",
    "\n",
    "Check the number of topologies loaded to ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26132"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the number of topologies loaded\n",
    "# This should match the output from the anomaly detection step\n",
    "len(Topologies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABEL RULE 1: Match with Known Faults\n",
    "\n",
    "This rule matches topologies with known injected faults and platform faults to create ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Process Injected Faults\n",
    "\n",
    "Load and process the injected fault data to identify which topologies correspond to real incidents. Injected faults are controlled experiments where faults were deliberately introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the injected fault data\n",
    "# This contains information about deliberately introduced faults for testing\n",
    "inject_df = pd.read_csv('../data/injected_faults.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize Labels\n",
    "\n",
    "Initialize all topologies with label 0 (normal operation). Labels will be updated based on the rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all topologies with label 0 (normal/non-incident)\n",
    "# Labels will be updated based on the labeling rules\n",
    "for topo in Topologies:\n",
    "    topo['y'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Apply Injected Fault Labels\n",
    "\n",
    "For each injected fault, find corresponding topologies and apply appropriate labels:\n",
    "- Find topologies within 15 minutes of fault injection\n",
    "- Label the most severe topology as incident (y=1)\n",
    "- Label other related topologies as undetermined (y=-1)\n",
    "\n",
    "**Special handling for 'excessive flow' faults**: These affect the entire system, so any service can be the root cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each injected fault to label corresponding topologies\n",
    "for index, inject_item in inject_df.iterrows():\n",
    "    # Find topologies that occur within 15 minutes after fault injection\n",
    "    # This time window captures the fault's impact period\n",
    "    corressponding_topo_i_s = [\n",
    "        i for i, topo in enumerate(Topologies) \n",
    "        if topo['time'] >= inject_item['time'] and topo['time'] < inject_item['time'] + 15\n",
    "    ]\n",
    "    \n",
    "    # Handle 'excessive flow' faults (system-wide impact)\n",
    "    if inject_item['inject_type'] == 'excessive flow':\n",
    "        MaxFail, MaxFail_ci = 0, None\n",
    "        \n",
    "        # Find the topology with maximum failure count (most severe impact)\n",
    "        for ci in corressponding_topo_i_s:\n",
    "            if Topologies[ci]['MaxFail'] > MaxFail:\n",
    "                MaxFail_ci = ci\n",
    "                MaxFail = Topologies[ci]['MaxFail']\n",
    "            # Mark all related topologies as undetermined first\n",
    "            Topologies[ci]['y'] = -1\n",
    "        \n",
    "        # Label the most severe topology as a true incident\n",
    "        if MaxFail_ci is not None:\n",
    "            Topologies[MaxFail_ci]['y'] = 1\n",
    "            Topologies[MaxFail_ci]['root_cause'] = 'All'  # System-wide fault\n",
    "            Topologies[MaxFail_ci]['root_cause_type'] = inject_item['inject_type']\n",
    "    \n",
    "    # Handle service-specific faults\n",
    "    else:\n",
    "        MaxFail, MaxFail_ci = 0, None\n",
    "        \n",
    "        # Find the most severe topology that involves the faulty service\n",
    "        for ci in corressponding_topo_i_s:\n",
    "            # Check if the injected service is involved in this topology\n",
    "            if (Topologies[ci]['MaxFail'] > MaxFail and \n",
    "                inject_item['inject_serive'] in Topologies[ci]['nodes']):\n",
    "                MaxFail_ci = ci\n",
    "                MaxFail = Topologies[ci]['MaxFail']\n",
    "            # Mark all related topologies as undetermined first\n",
    "            Topologies[ci]['y'] = -1\n",
    "        \n",
    "        # Label the most severe relevant topology as a true incident\n",
    "        if MaxFail_ci is not None:\n",
    "            Topologies[MaxFail_ci]['y'] = 1\n",
    "            Topologies[MaxFail_ci]['root_cause'] = inject_item['inject_serive']\n",
    "            Topologies[MaxFail_ci]['root_cause_type'] = inject_item['inject_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Process Platform Faults\n",
    "\n",
    "Platform faults are infrastructure-level issues that affect service operation. These are processed similarly to injected faults but use timestamp ranges instead of fixed time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load platform fault data\n",
    "# These are infrastructure-level faults that affect service operation\n",
    "platform_fault_df = pd.read_csv('../data/platform_faults.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Apply Platform Fault Labels\n",
    "\n",
    "Process platform faults using their begin and end timestamps to find affected topologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21304\n",
      "22570\n"
     ]
    }
   ],
   "source": [
    "# Process each platform fault\n",
    "for index, platform_fault in platform_fault_df.iterrows():\n",
    "    # Find topologies within the platform fault time window\n",
    "    # Use timestamp comparison for more precise matching\n",
    "    corressponding_topo_i_s = [\n",
    "        i for i, topo in enumerate(Topologies) \n",
    "        if (pd.to_datetime(topo['TimeStamp']) >= pd.to_datetime(platform_fault['BeginTimeStamp']) and \n",
    "            pd.to_datetime(topo['TimeStamp']) <= pd.to_datetime(platform_fault['EndTimeStamp']))\n",
    "    ]\n",
    "    \n",
    "    MaxFail, MaxFail_ci = 0, None\n",
    "    \n",
    "    # Find the most severe topology involving the affected service\n",
    "    for ci in corressponding_topo_i_s:\n",
    "        # Only consider topologies that:\n",
    "        # 1. Involve the affected service\n",
    "        # 2. Haven't already been labeled as incidents (y != 1)\n",
    "        # 3. Have higher failure count than current maximum\n",
    "        if (Topologies[ci]['MaxFail'] > MaxFail and \n",
    "            platform_fault['service'] in Topologies[ci]['nodes'] and \n",
    "            Topologies[ci]['y'] != 1):\n",
    "            MaxFail_ci = ci\n",
    "            MaxFail = Topologies[ci]['MaxFail']\n",
    "        \n",
    "        # Mark as undertermined if not already labeled as incident\n",
    "        if Topologies[ci]['y'] != 1:\n",
    "            Topologies[ci]['y'] = -1\n",
    "    \n",
    "    # Label the most severe topology as incident\n",
    "    if MaxFail_ci is not None:\n",
    "        Topologies[MaxFail_ci]['y'] = 1\n",
    "        Topologies[MaxFail_ci]['root_cause'] = platform_fault['service']\n",
    "        Topologies[MaxFail_ci]['root_cause_type'] = 'platform_fault'\n",
    "        print(MaxFail_ci)  # Print index for tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABEL RULE 2: Business Logic Application\n",
    "\n",
    "Apply business-specific rules based on system design. Services with degradation policies can handle faults gracefully without affecting core functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Apply Degradation Policy Rule\n",
    "\n",
    "Services like 'adservice' and 'emailservice' have degradation policies implemented. When these services fail, the system continues to function with reduced features rather than complete failure. Therefore, incidents involving only these services are reclassified as non-incidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply business logic: services with degradation policies\n",
    "# These services have fallback mechanisms that prevent system-wide impact\n",
    "for topo in Topologies:\n",
    "    # If the root cause is a service with degradation policy,\n",
    "    # reclassify as non-incident since the system handles it gracefully\n",
    "    if (topo['y'] == 1 and \n",
    "        (topo['root_cause'] == 'adservice' or topo['root_cause'] == 'emailservice')):\n",
    "        topo['y'] = 0  # Change from incident to normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Check Label Distribution After Rule 2\n",
    "\n",
    "Verify how the label distribution changed after applying the degradation policy rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    20012\n",
       " 0     5417\n",
       " 1      703\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the distribution of labels after applying Rule 2\n",
    "# This shows how many incidents were reclassified due to degradation policies\n",
    "pd.Series([topo['y'] for topo in Topologies]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABEL RULE 3: Filter Low-Impact Issues\n",
    "\n",
    "Performance issues during low-traffic periods (e.g., 00:00-06:00) often have minimal impact and may not require immediate attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Apply Low-Impact Filter\n",
    "\n",
    "Filter out performance issues (CPU, latency) that occur during low-traffic periods with low failure counts. These issues, while technically anomalous, don't significantly impact user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out low-impact performance issues\n",
    "# Performance issues with low failure counts during low-traffic periods\n",
    "# are often not significant enough to be considered incidents\n",
    "for topo in Topologies:\n",
    "    # Check if this is a performance-related incident with low impact\n",
    "    if (topo['y'] == 1 and \n",
    "        (topo['root_cause_type'] == 'cpu' or topo['root_cause_type'] == 'latency') and \n",
    "        topo['MaxFail'] < 50):  # Threshold for \"low impact\"\n",
    "        topo['y'] = 0  # Reclassify as normal operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Final Label Distribution\n",
    "\n",
    "Check the final distribution of labels after applying all three labeling rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    20012\n",
       " 0     5494\n",
       " 1      626\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display final label distribution\n",
    "# -1: Non-incident anomalies\n",
    "#  0: Normal operation\n",
    "#  1: True incidents requiring attention\n",
    "pd.Series([topo['y'] for topo in Topologies]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Save Labeled Data\n",
    "\n",
    "Save the labeled topologies for use in feature engineering and model training. This dataset now contains ground truth labels for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the labeled topologies\n",
    "# This labeled dataset will be used for:\n",
    "# 1. Feature engineering\n",
    "# 2. Training machine learning models\n",
    "# 3. Evaluating incident detection performance\n",
    "with open('../data/issue_topoloies.pkl', 'wb') as f:\n",
    "    pickle.dump(Topologies, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
