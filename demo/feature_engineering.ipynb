{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Incident Detection\n",
    "\n",
    "This notebook transforms the labeled topology data into machine learning-ready features. The process includes:\n",
    "\n",
    "1. **Node Feature Engineering**: Create features for each service using CMDB information\n",
    "2. **Edge Feature Engineering**: Extract time-series and statistical features from service call relationships\n",
    "3. **Global Feature Engineering**: Create topology-level features\n",
    "4. **Data Normalization**: Standardize features for machine learning\n",
    "5. **Graph Structure Preparation**: Prepare data for graph neural networks\n",
    "\n",
    "The output will be training and testing datasets ready for incident detection models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "Import libraries for data processing, feature engineering, and machine learning preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data serialization and file I/O\n",
    "import pickle\n",
    "\n",
    "# Data processing and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Machine learning preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Labeled Topology Data\n",
    "\n",
    "Load the labeled topologies from the data labeling step. These contain the ground truth labels and topology structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labeled topology data\n",
    "# This data contains topologies with ground truth labels (incident vs non-incident)\n",
    "with open('../data/issue_topoloies.pkl', 'rb') as f:\n",
    "    Topologies = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Filter Valid Cases\n",
    "\n",
    "Extract only the cases that are labeled as either incidents (y=1) or normal operations (y=0), excluding the anomalies that were marked as undetermined (y=-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out cases labeled as -1 (anomalies that are not incidents)\n",
    "# We only want cases labeled as 0 (normal) or 1 (incident) for binary classification\n",
    "all_cases = [case for case in Topologies if case['y'] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify Dataset Size\n",
    "\n",
    "Check the number of valid cases for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6120"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the number of valid cases for feature engineering\n",
    "# This should be significantly smaller than the original topology count\n",
    "len(all_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Service CMDB (Configuration Management Database)\n",
    "\n",
    "Define service metadata that will be used as node features. This includes:\n",
    "- **Fault Tolerance Type**: How the service handles failures (retry, degrade, none)\n",
    "- **Importance Level**: Business criticality of the service\n",
    "- **Product Name**: Which product/domain the service belongs to\n",
    "- **Status**: Current operational status\n",
    "- **Type Name**: Technical category of the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the service metadata dictionary\n",
    "# This acts as a Configuration Management Database (CMDB) for our services\n",
    "nodes_cmdb = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define service characteristics based on system architecture\n",
    "\n",
    "# Fault tolerance mechanisms implemented by each service\n",
    "nodes_cmdb['fault_tolerance_type'] = {\n",
    "    'frontend': 'no',                    # No special fault tolerance\n",
    "    'cartservice': 'retry',              # Implements retry logic\n",
    "    'productcatalogservice': 'retry',    # Implements retry logic\n",
    "    'currencyservice': 'retry',          # Implements retry logic\n",
    "    'paymentservice': 'retry',           # Implements retry logic\n",
    "    'shippingservice': 'retry',          # Implements retry logic\n",
    "    'emailservice': 'degrade',           # Graceful degradation\n",
    "    'checkoutservice': 'retry',          # Implements retry logic\n",
    "    'recommendationservice': 'retry',    # Implements retry logic\n",
    "    'adservice': 'degrade',              # Graceful degradation\n",
    "    'mysql': 'retry',                    # Database retry mechanisms\n",
    "    'redis-cart': 'retry'                # Cache retry mechanisms\n",
    "}\n",
    "\n",
    "# Business importance level of each service\n",
    "nodes_cmdb['importance_level'] = {\n",
    "    'frontend': 'important',             # User-facing interface\n",
    "    'cartservice': 'important',          # Core shopping functionality\n",
    "    'productcatalogservice': 'important', # Core product data\n",
    "    'currencyservice': 'important',      # Financial calculations\n",
    "    'paymentservice': 'important',       # Payment processing\n",
    "    'shippingservice': 'important',      # Order fulfillment\n",
    "    'emailservice': 'ordinary',          # Non-critical notifications\n",
    "    'checkoutservice': 'important',      # Core purchase flow\n",
    "    'recommendationservice': 'important', # User experience\n",
    "    'adservice': 'important',            # Revenue generation\n",
    "    'mysql': 'important',                # Primary data store\n",
    "    'redis-cart': 'ordinary'             # Cache layer\n",
    "}\n",
    "\n",
    "# Product domain classification\n",
    "nodes_cmdb['product_name'] = {\n",
    "    'frontend': 'basic',                 # Basic infrastructure\n",
    "    'cartservice': 'shopping',           # Shopping domain\n",
    "    'productcatalogservice': 'basic',    # Basic infrastructure\n",
    "    'currencyservice': 'shopping',       # Shopping domain\n",
    "    'paymentservice': 'shopping',        # Shopping domain\n",
    "    'shippingservice': 'shopping',       # Shopping domain\n",
    "    'emailservice': 'shopping',          # Shopping domain\n",
    "    'checkoutservice': 'shopping',       # Shopping domain\n",
    "    'recommendationservice': 'recommendation', # Recommendation domain\n",
    "    'adservice': 'ad',                   # Advertisement domain\n",
    "    'mysql': 'basic',                    # Basic infrastructure\n",
    "    'redis-cart': 'shopping'             # Shopping domain\n",
    "}\n",
    "\n",
    "# Operational status (all services are currently online)\n",
    "nodes_cmdb['status'] = {service: 'online' for service in nodes_cmdb['fault_tolerance_type'].keys()}\n",
    "\n",
    "# Technical service type classification\n",
    "nodes_cmdb['type_name'] = {\n",
    "    'frontend': 'httpapi',               # HTTP API gateway\n",
    "    'cartservice': 'appsvr',             # Application server\n",
    "    'productcatalogservice': 'appsvr',   # Application server\n",
    "    'currencyservice': 'appsvr',         # Application server\n",
    "    'paymentservice': 'appsvr',          # Application server\n",
    "    'shippingservice': 'appsvr',         # Application server\n",
    "    'emailservice': 'appsvr',            # Application server\n",
    "    'checkoutservice': 'appsvr',         # Application server\n",
    "    'recommendationservice': 'appsvr',   # Application server\n",
    "    'adservice': 'appsvr',               # Application server\n",
    "    'mysql': 'mysql',                    # Database server\n",
    "    'redis-cart': 'cache'                # Cache server\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Convert CMDB to DataFrame and One-Hot Encode\n",
    "\n",
    "Transform the service metadata into a structured format and apply one-hot encoding to categorical variables for machine learning compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the CMDB dictionary to a pandas DataFrame\n",
    "# Each row represents a service, each column represents a characteristic\n",
    "nodes_cmdb = pd.DataFrame(nodes_cmdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to categorical variables\n",
    "# This converts categorical features into binary vectors for ML algorithms\n",
    "# Each unique category becomes a separate binary feature\n",
    "nodes_cmdb = pd.get_dummies(nodes_cmdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Node and Edge Features for Graph Structure\n",
    "\n",
    "Transform each topology case into a graph structure with:\n",
    "- **Node features**: Service characteristics from CMDB\n",
    "- **Edge indices**: Connections between services\n",
    "- **Node mapping**: Service name to node index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph structure for each topology case\n",
    "for case in all_cases:\n",
    "    # Initialize mapping from service names to node indices\n",
    "    nodes_map = {}\n",
    "    nodes_num = 0\n",
    "    nodes_feature = []  # Node feature matrix\n",
    "    edge_index = []     # Edge connectivity list\n",
    "    \n",
    "    # Process each edge in the topology\n",
    "    for edge in case['edges_info']:\n",
    "        # Add source node if not already present\n",
    "        if edge['src'] not in nodes_map:\n",
    "            nodes_map[edge['src']] = nodes_num\n",
    "            # Get node features from CMDB (one-hot encoded service characteristics)\n",
    "            nodes_feature.append(list(nodes_cmdb.loc[edge['src']].values))\n",
    "            nodes_num += 1\n",
    "        \n",
    "        # Add destination node if not already present\n",
    "        if edge['des'] not in nodes_map:\n",
    "            nodes_map[edge['des']] = nodes_num\n",
    "            # Get node features from CMDB\n",
    "            nodes_feature.append(list(nodes_cmdb.loc[edge['des']].values))\n",
    "            nodes_num += 1\n",
    "        \n",
    "        # Add edge connection using node indices\n",
    "        edge_index.append([nodes_map[edge['src']], nodes_map[edge['des']]])\n",
    "    \n",
    "    # Store graph structure in the case\n",
    "    case['x'] = nodes_feature      # Node feature matrix\n",
    "    case['edge_index'] = edge_index # Edge connectivity list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Split Data into Training and Testing Sets\n",
    "\n",
    "Divide the dataset into training and testing portions for model development and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (50-50 split)\n",
    "# First half for training\n",
    "train_cases = all_cases[:len(all_cases)//2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second half for testing\n",
    "test_cases = all_cases[len(all_cases)//2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Extract Edge Features from Time Series Data\n",
    "\n",
    "Create sophisticated edge features from the time series data of each service call relationship. These features capture:\n",
    "\n",
    "1. **Failure Level Binning**: Categorize failure severity\n",
    "2. **Temporal Ratios**: Compare current vs historical failure rates\n",
    "3. **Workload Ratios**: Failure rate relative to request volume\n",
    "4. **Trend Analysis**: Recent failure pattern changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract edge features for training data\n",
    "for case in train_cases:\n",
    "    case['edge_fea'] = []\n",
    "    \n",
    "    for i, edge_attr in enumerate(case['edges_info']):\n",
    "        case['edge_fea'].append([])\n",
    "        \n",
    "        # Feature 1: Failure Level Binning\n",
    "        # Categorize the current failure level into severity bins\n",
    "        final_fail_level = np.max(edge_attr['FailCount'][-1:])\n",
    "        \n",
    "        # Binning thresholds are system-specific\n",
    "        # For larger systems, use higher thresholds (e.g., [5000, 10000, 30000, 70000])\n",
    "        if final_fail_level < 50:\n",
    "            case['edge_fea'][i].append(0)                # Low severity\n",
    "        elif final_fail_level < 100:\n",
    "            case['edge_fea'][i].append(1e2 - 1)          # Medium-low severity\n",
    "        elif final_fail_level < 300:\n",
    "            case['edge_fea'][i].append(1e4 - 1)          # Medium severity\n",
    "        elif final_fail_level < 700:\n",
    "            case['edge_fea'][i].append(1e7 - 1)          # High severity\n",
    "        else:\n",
    "            case['edge_fea'][i].append(1e10 - 1)         # Critical severity\n",
    "        \n",
    "        # Feature 2-4: Temporal Comparison Ratios\n",
    "        # Compare current failure rate with different historical periods\n",
    "        points_length = len(edge_attr['FailCount'])\n",
    "        \n",
    "        # Current vs early period ratio\n",
    "        early_period_mean = np.mean(edge_attr['FailCount'][:-points_length//2])\n",
    "        current_vs_early = (np.max(edge_attr['FailCount'][-1:]) + 1) / (early_period_mean + 1)\n",
    "        case['edge_fea'][i].append(current_vs_early)\n",
    "        \n",
    "        # Current vs middle period ratio\n",
    "        middle_period_mean = np.mean(edge_attr['FailCount'][-points_length//2:-points_length//4])\n",
    "        current_vs_middle = (np.max(edge_attr['FailCount'][-1:]) + 1) / (middle_period_mean + 1)\n",
    "        case['edge_fea'][i].append(current_vs_middle)\n",
    "        \n",
    "        # Current vs yesterday same time ratio\n",
    "        yesterday_max = np.max(edge_attr['YesterFailCount'][points_length//2:])\n",
    "        current_vs_yesterday = (np.max(edge_attr['FailCount'][-1:]) + 1) / (yesterday_max + 1)\n",
    "        case['edge_fea'][i].append(current_vs_yesterday)\n",
    "        \n",
    "        # Feature 5: Failure Rate vs Workload\n",
    "        # Calculate failure rate relative to request volume\n",
    "        current_workload = np.array(edge_attr['Workload'])[-1:]\n",
    "        current_failures = np.array(edge_attr['FailCount'])[-1:]\n",
    "        failure_rate = np.mean((current_failures + 1) / (current_workload + 1))\n",
    "        case['edge_fea'][i].append(failure_rate)\n",
    "        \n",
    "        # Feature 6: Current vs Yesterday Failure Rate\n",
    "        # Compare today's failure rate with yesterday's\n",
    "        current_vs_yesterday_rate = np.mean(\n",
    "            ((np.array(edge_attr['FailCount']) + 1) / \n",
    "            (np.array(edge_attr['YesterFailCount']) + 1))[-1:]\n",
    "        )\n",
    "        case['edge_fea'][i].append(current_vs_yesterday_rate)\n",
    "        \n",
    "        # Feature 7: Recent Trend Analysis\n",
    "        # Compare recent peak with current level to detect trends\n",
    "        recent_peak = np.max(np.array(edge_attr['FailCount'])[-4:-1])\n",
    "        current_level = edge_attr['FailCount'][-1]\n",
    "        trend_ratio = (recent_peak + 1) / (current_level + 1)\n",
    "        case['edge_fea'][i].append(trend_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Extract Edge Features for Test Data\n",
    "\n",
    "Apply the same feature extraction process to the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract edge features for test data using the same logic\n",
    "# (Same code as training data feature extraction)\n",
    "for case in test_cases:\n",
    "    case['edge_fea'] = []\n",
    "    \n",
    "    for i, edge_attr in enumerate(case['edges_info']):\n",
    "        case['edge_fea'].append([])\n",
    "        \n",
    "        # Apply the same feature extraction logic as training data\n",
    "        final_fail_level = np.max(edge_attr['FailCount'][-1:])\n",
    "        \n",
    "        if final_fail_level < 50:\n",
    "            case['edge_fea'][i].append(0)\n",
    "        elif final_fail_level < 100:\n",
    "            case['edge_fea'][i].append(1e2 - 1)\n",
    "        elif final_fail_level < 300:\n",
    "            case['edge_fea'][i].append(1e4 - 1)\n",
    "        elif final_fail_level < 700:\n",
    "            case['edge_fea'][i].append(1e7 - 1)\n",
    "        else:\n",
    "            case['edge_fea'][i].append(1e10 - 1)\n",
    "        \n",
    "        points_length = len(edge_attr['FailCount'])\n",
    "        case['edge_fea'][i].append((np.max(edge_attr['FailCount'][-1:]) + 1)/(np.mean(edge_attr['FailCount'][:-points_length//2])+1))\n",
    "        case['edge_fea'][i].append((np.max(edge_attr['FailCount'][-1:]) + 1)/(np.mean(edge_attr['FailCount'][-points_length//2:-points_length//4])+1))\n",
    "        case['edge_fea'][i].append((np.max(edge_attr['FailCount'][-1:]) + 1)/(np.max(edge_attr['YesterFailCount'][points_length//2:])+1))\n",
    "        case['edge_fea'][i].append(np.mean(((np.array(edge_attr['FailCount'])+1)/(np.array(edge_attr['Workload'])+1))[-1:]))\n",
    "        case['edge_fea'][i].append(np.mean(((np.array(edge_attr['FailCount'])+1)/(np.array(edge_attr['YesterFailCount'])+1))[-1:]))\n",
    "        case['edge_fea'][i].append((np.max(np.array(edge_attr['FailCount'])[-4:-1])+1)/(edge_attr['FailCount'][-1]+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Apply Logarithmic Transformation\n",
    "\n",
    "Apply log transformation to edge features to handle large value ranges and improve model stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply logarithmic transformation to training edge features\n",
    "# This helps normalize large value ranges and improves model convergence\n",
    "for case in train_cases:\n",
    "    for i, edge_fea in enumerate(case['edge_fea']):\n",
    "        # Apply log base 10 transformation (add 1 to avoid log(0))\n",
    "        case['edge_fea'][i] = [math.log(f + 1, 10) for f in edge_fea]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply logarithmic transformation to test edge features\n",
    "for case in test_cases:\n",
    "    for i, edge_fea in enumerate(case['edge_fea']):\n",
    "        case['edge_fea'][i] = [math.log(f + 1, 10) for f in edge_fea]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Standardize Edge Features\n",
    "\n",
    "Apply standardization (z-score normalization) to edge features using training data statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all edge features from training data for standardization\n",
    "# This ensures we use only training data statistics for normalization\n",
    "all_fea_for_norm = []\n",
    "for case in train_cases:\n",
    "    for i, edge_fea in enumerate(case['edge_fea']):\n",
    "        all_fea_for_norm.append(edge_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.42218303, -0.3151539 , -0.45919992, ..., -0.49623995,\n",
       "        -0.10882742, -0.1122749 ],\n",
       "       [-0.42218303, -0.57792549, -0.68802945, ..., -0.50003458,\n",
       "        -0.43365717,  1.02474846],\n",
       "       [-0.42218303, -0.32928518, -0.68802945, ..., -0.50130648,\n",
       "        -0.43365717,  0.51993701],\n",
       "       ...,\n",
       "       [-0.42218303, -0.44279358, -0.52983737, ..., -0.46424418,\n",
       "        -0.58531977,  0.29108647],\n",
       "       [-0.42218303, -0.44430415, -0.51369982, ..., -0.4926088 ,\n",
       "        -0.52997535, -0.9963308 ],\n",
       "       [-0.42218303, -0.78025256, -0.85850706, ..., -0.48451811,\n",
       "        -0.8465918 ,  2.79943788]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit StandardScaler on training data\n",
    "# This calculates mean and standard deviation for each feature\n",
    "ss = StandardScaler()\n",
    "ss.fit_transform(all_fea_for_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standardization to training edge features\n",
    "for case in train_cases:\n",
    "    case['edge_fea'] = ss.transform(case['edge_fea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same standardization to test edge features\n",
    "# Important: Use training data statistics, not test data statistics\n",
    "for case in test_cases:\n",
    "    case['edge_fea'] = ss.transform(case['edge_fea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Create Global Topology Features\n",
    "\n",
    "Extract topology-level features that characterize the overall structure and severity of each incident topology:\n",
    "\n",
    "1. **Number of Services**: Topology size (log-transformed)\n",
    "2. **Number of Relationships**: Topology complexity (log-transformed)\n",
    "3. **Important Services Count**: Number of business-critical services involved\n",
    "4. **Overall Severity Level**: Binned total failure count across the topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract global features for training data\n",
    "for case in train_cases:\n",
    "    # Feature 1: Log of number of services in topology\n",
    "    num_services = math.log(len(case['x']) + 1, 10)\n",
    "    \n",
    "    # Feature 2: Log of number of calling relationships\n",
    "    num_relationships = math.log(len(case['edge_index']) + 1, 10)\n",
    "    \n",
    "    # Feature 3: Number of important services involved\n",
    "    # Index 3 corresponds to 'importance_level_important' from one-hot encoding\n",
    "    num_important_services = np.sum([node_x[3] for node_x in case['x']])\n",
    "    \n",
    "    case['global_fea'] = [num_services, num_relationships, num_important_services]\n",
    "\n",
    "# Standardize global features using training data\n",
    "global_fea_for_norm = [case['global_fea'] for case in train_cases]\n",
    "ss_global = StandardScaler()\n",
    "norm_rst = ss_global.fit_transform(global_fea_for_norm)\n",
    "\n",
    "# Apply standardized global features to training data\n",
    "for i, case in enumerate(train_cases):\n",
    "    case['global_fea'] = norm_rst[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and standardize global features for test data\n",
    "for case in test_cases:\n",
    "    # Apply same feature extraction logic\n",
    "    num_services = math.log(len(case['x']) + 1, 10)\n",
    "    num_relationships = math.log(len(case['edge_index']) + 1, 10)\n",
    "    num_important_services = np.sum([node_x[3] for node_x in case['x']])\n",
    "    \n",
    "    case['global_fea'] = [num_services, num_relationships, num_important_services]\n",
    "\n",
    "# Apply training data standardization to test data\n",
    "for i, case in enumerate(test_cases):\n",
    "    case['global_fea'] = ss_global.transform(np.array(case['global_fea']).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Add Overall Severity Feature\n",
    "\n",
    "Add a binned feature representing the overall severity level of the topology based on total failure count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add overall severity level feature to training data\n",
    "for case in train_cases:\n",
    "    # Calculate total failure count across all edges in the topology\n",
    "    sum_fail = 0\n",
    "    for i, edge_attr in enumerate(case['edges_info']):\n",
    "        sum_fail += np.mean(edge_attr['FailCount'][-1:])\n",
    "    \n",
    "    # Bin the total failure count into severity levels\n",
    "    if sum_fail < 50:\n",
    "        severity_level = 0      # Very low severity\n",
    "    elif sum_fail < 100:\n",
    "        severity_level = 0.2    # Low severity\n",
    "    elif sum_fail < 150:\n",
    "        severity_level = 0.4    # Medium severity\n",
    "    elif sum_fail < 700:\n",
    "        severity_level = 0.7    # High severity\n",
    "    else:\n",
    "        severity_level = 1      # Critical severity\n",
    "    \n",
    "    # Append severity level to global features\n",
    "    case['global_fea'] = np.append(case['global_fea'], severity_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add overall severity level feature to test data\n",
    "for case in test_cases:\n",
    "    # Apply same severity binning logic\n",
    "    sum_fail = 0\n",
    "    for i, edge_attr in enumerate(case['edges_info']):\n",
    "        sum_fail += np.mean(edge_attr['FailCount'][-1:])\n",
    "    \n",
    "    if sum_fail < 50:\n",
    "        severity_level = 0\n",
    "    elif sum_fail < 100:\n",
    "        severity_level = 0.2\n",
    "    elif sum_fail < 150:\n",
    "        severity_level = 0.4\n",
    "    elif sum_fail < 700:\n",
    "        severity_level = 0.7\n",
    "    else:\n",
    "        severity_level = 1\n",
    "    \n",
    "    case['global_fea'] = np.append(case['global_fea'], severity_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Create Bidirectional Graph Structure\n",
    "\n",
    "Convert directed edges to bidirectional edges for graph neural network processing. This allows information to flow in both directions along service call relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bidirectional edge indices for training data\n",
    "# Add reverse edges to allow bidirectional information flow\n",
    "for case in train_cases:\n",
    "    # Original edges + reverse edges\n",
    "    case['bi_edge_index'] = case['edge_index'] + [[edge[1], edge[0]] for edge in case['edge_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bidirectional edge indices for test data\n",
    "for case in test_cases:\n",
    "    case['bi_edge_index'] = case['edge_index'] + [[edge[1], edge[0]] for edge in case['edge_index']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Create Bidirectional Edge Features\n",
    "\n",
    "Duplicate edge features to match the bidirectional edge structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate edge features for bidirectional edges in training data\n",
    "# Each edge feature is used for both directions\n",
    "for case in train_cases:\n",
    "    case['bi_edge_fea'] = np.concatenate((case['edge_fea'], case['edge_fea']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate edge features for bidirectional edges in test data\n",
    "for case in test_cases:\n",
    "    case['bi_edge_fea'] = np.concatenate((case['edge_fea'], case['edge_fea']), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Save Processed Datasets\n",
    "\n",
    "Save the feature-engineered training and testing datasets for use in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed training dataset\n",
    "# This dataset is ready for machine learning model training\n",
    "with open('../data/train_cases.pkl', 'wb') as f:\n",
    "    pickle.dump(train_cases, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed test dataset\n",
    "# This dataset is ready for model evaluation\n",
    "with open('../data/test_cases.pkl', 'wb') as f:\n",
    "    pickle.dump(test_cases, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
