{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection and Impact Extraction\n",
    "\n",
    "This notebook demonstrates the process of detecting anomalies in microservice systems and extracting their impact topologies. The workflow includes:\n",
    "1. **Data Loading**: Load monitoring data from calling relationships\n",
    "2. **Data Preprocessing**: Organize service pair data and add historical context\n",
    "3. **Anomaly Detection**: Identify anomalous behavior in service communications\n",
    "4. **Impact Extraction**: Extract topology structures representing incident impacts\n",
    "\n",
    "The output will be raw topology data that can be used for further analysis and incident diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "We import essential libraries for data processing, clustering, visualization, and graph analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing and analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning for clustering anomalous patterns\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Visualization and graph analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Data serialization and file I/O\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Monitoring Data\n",
    "\n",
    "Load the calling relationships monitoring data which contains:\n",
    "- **TimeStamp**: When the measurement was taken\n",
    "- **SourceName**: The calling service\n",
    "- **DestinationName**: The called service\n",
    "- **Workload**: Number of requests/calls\n",
    "- **FailCount**: Number of failed requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the monitoring data containing service call relationships and metrics\n",
    "all_data = pd.read_csv(\"../data/calling_relationships_monitoring.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the Dataset\n",
    "\n",
    "Display the dataset to understand its structure and content. This helps us verify the data format and identify any potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>SourceName</th>\n",
       "      <th>DestinationName</th>\n",
       "      <th>Workload</th>\n",
       "      <th>FailCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-23 15:55:00</td>\n",
       "      <td>frontend</td>\n",
       "      <td>adservice</td>\n",
       "      <td>664.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-23 15:55:00</td>\n",
       "      <td>frontend</td>\n",
       "      <td>checkoutservice</td>\n",
       "      <td>54.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-03-23 15:55:00</td>\n",
       "      <td>frontend</td>\n",
       "      <td>shippingservice</td>\n",
       "      <td>250.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-03-23 15:55:00</td>\n",
       "      <td>frontend</td>\n",
       "      <td>currencyservice</td>\n",
       "      <td>3653.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-03-23 15:55:00</td>\n",
       "      <td>frontend</td>\n",
       "      <td>productcatalogservice</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281355</th>\n",
       "      <td>2022-04-04 20:59:00</td>\n",
       "      <td>checkoutservice</td>\n",
       "      <td>productcatalogservice</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281356</th>\n",
       "      <td>2022-04-04 20:59:00</td>\n",
       "      <td>checkoutservice</td>\n",
       "      <td>cartservice</td>\n",
       "      <td>98.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281357</th>\n",
       "      <td>2022-04-04 20:59:00</td>\n",
       "      <td>recommendationservice</td>\n",
       "      <td>productcatalogservice</td>\n",
       "      <td>1001.333333</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281358</th>\n",
       "      <td>2022-04-04 20:59:00</td>\n",
       "      <td>cartservice</td>\n",
       "      <td>redis-cart</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281359</th>\n",
       "      <td>2022-04-04 20:59:00</td>\n",
       "      <td>productcatalogservice</td>\n",
       "      <td>mysql</td>\n",
       "      <td>3801.333333</td>\n",
       "      <td>14.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281360 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  TimeStamp             SourceName        DestinationName  \\\n",
       "0       2022-03-23 15:55:00               frontend              adservice   \n",
       "1       2022-03-23 15:55:00               frontend        checkoutservice   \n",
       "2       2022-03-23 15:55:00               frontend        shippingservice   \n",
       "3       2022-03-23 15:55:00               frontend        currencyservice   \n",
       "4       2022-03-23 15:55:00               frontend  productcatalogservice   \n",
       "...                     ...                    ...                    ...   \n",
       "281355  2022-04-04 20:59:00        checkoutservice  productcatalogservice   \n",
       "281356  2022-04-04 20:59:00        checkoutservice            cartservice   \n",
       "281357  2022-04-04 20:59:00  recommendationservice  productcatalogservice   \n",
       "281358  2022-04-04 20:59:00            cartservice             redis-cart   \n",
       "281359  2022-04-04 20:59:00  productcatalogservice                  mysql   \n",
       "\n",
       "           Workload  FailCount  \n",
       "0        664.000000   0.000000  \n",
       "1         54.666667   0.000000  \n",
       "2        250.666667   0.000000  \n",
       "3       3653.333333   0.000000  \n",
       "4       5172.000000   0.000000  \n",
       "...             ...        ...  \n",
       "281355   104.000000   0.000000  \n",
       "281356    98.666667   0.000000  \n",
       "281357  1001.333333  20.000000  \n",
       "281358   736.000000   0.000000  \n",
       "281359  3801.333333  14.666667  \n",
       "\n",
       "[281360 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the dataset overview\n",
    "# This shows us the structure: timestamps, service pairs, workloads, and failure counts\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Organize Data by Service Pairs\n",
    "\n",
    "Group the data by service pairs (source-destination combinations) and add historical context:\n",
    "- Create separate time series for each service pair\n",
    "- Add yesterday's failure count as a baseline for comparison (shift by 1440 minutes = 24 hours)\n",
    "- This historical context helps identify unusual patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store time series data for each service pair\n",
    "ServicePairs = {}\n",
    "\n",
    "# Group data by source and destination service names\n",
    "for g in all_data.groupby(['SourceName', 'DestinationName']):\n",
    "    # g[0] contains the group key (source, destination tuple)\n",
    "    # g[1] contains the grouped data for this service pair\n",
    "    ServicePairs[g[0]] = g[1].reset_index(drop=True)\n",
    "    \n",
    "    # Add historical context: yesterday's failure count for comparison\n",
    "    # Shift by 1440 minutes (24 hours) to get the same time yesterday\n",
    "    # Fill missing values with 0 for the first day\n",
    "    ServicePairs[g[0]]['YesterFailCount'] = ServicePairs[g[0]]['FailCount'].shift(1440).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection\n",
    "\n",
    "This section implements a simple anomaly detection mechanism. In production environments, more sophisticated anomaly detectors would be used for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Simple Anomaly Detection\n",
    "\n",
    "**Note**: This is a simplified anomaly detector for demonstration purposes. In practice, more advanced methods would be used.\n",
    "\n",
    "The current approach:\n",
    "- Considers any non-zero failure count as an anomaly\n",
    "- This is suitable for this simulation dataset where failures are explicitly injected\n",
    "- Real-world systems would use statistical methods, machine learning, or threshold-based approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create anomaly indicators for each service pair\n",
    "ServicePairsAnomalies = {}\n",
    "\n",
    "for pair in ServicePairs:\n",
    "    # Simple anomaly detection: any failure count > 0 is considered anomalous\n",
    "    # This works for simulation data where failures are explicitly injected\n",
    "    ServicePairsAnomalies[pair] = ServicePairs[pair]['FailCount'] > 0\n",
    "    \n",
    "    # Alternative approach (commented): require consecutive anomalies\n",
    "    # This would reduce false positives by requiring sustained anomalous behavior\n",
    "    # ServicePairsAnomalies[pair] = pd.Series(np.all([\n",
    "    #     ServicePairsAnomalies[pair],\n",
    "    #     ServicePairsAnomalies[pair].shift(1), \n",
    "    #     ServicePairsAnomalies[pair].shift(2)\n",
    "    # ], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Prepare System-Level Anomaly Data\n",
    "\n",
    "Combine individual service pair anomalies into a system-wide view to identify time periods when the system experiences issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all service pair keys for processing\n",
    "keys = list(ServicePairs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of time series data (should be same for all service pairs)\n",
    "datalen = len(ServicePairs[keys[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a system-wide anomaly matrix\n",
    "# Each row represents a time point, each column represents a service pair\n",
    "# True indicates an anomaly in that service pair at that time\n",
    "SystemAnomalies = pd.concat([ServicePairsAnomalies[k] for k in ServicePairsAnomalies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set column names to service pair identifiers\n",
    "SystemAnomalies.columns = [k for k in ServicePairsAnomalies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyze System-Level Anomaly Distribution\n",
    "\n",
    "Check how many time periods have at least one anomalous service pair. This gives us an overview of system health over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     11592\n",
       "False     5993\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count time periods with and without system-level anomalies\n",
    "# any(axis=1) returns True if any service pair has an anomaly at that time\n",
    "SystemAnomalies.any(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean series indicating time periods with any system anomaly\n",
    "# This will be used to identify when to extract impact topologies\n",
    "SystemAnomalies_any = SystemAnomalies.any(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact Extraction\n",
    "\n",
    "This section extracts topology structures that represent the impact of incidents on the system. The process identifies connected components of anomalous service pairs and creates topology features for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Extract Impact Topologies\n",
    "\n",
    "For each time period with anomalies, extract the topology structure representing the incident impact:\n",
    "\n",
    "**Process Overview:**\n",
    "1. **Single Edge Case**: If only one service pair is anomalous, create a simple topology\n",
    "2. **Multiple Edges Case**: Use correlation analysis and clustering to group related anomalies\n",
    "3. **Graph Analysis**: Use NetworkX to find connected components in the anomaly graph\n",
    "4. **Feature Extraction**: Extract time series features for each topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters for topology extraction\n",
    "THRESHOLD = 0.9  # Correlation threshold for grouping related anomalies\n",
    "min_sample = 1   # Minimum samples for DBSCAN clustering\n",
    "\n",
    "# List to store all extracted topologies\n",
    "Topologies = []\n",
    "\n",
    "# Process each time point (starting from index 9 to have enough historical data)\n",
    "for t in range(9, datalen-1):\n",
    "    # Only process time points where system has anomalies\n",
    "    if SystemAnomalies_any.loc[t]:\n",
    "        # Get list of service pairs that are anomalous at this time\n",
    "        anomalypairs = [k for k in keys if SystemAnomalies.loc[t][k]]\n",
    "        \n",
    "        # Case 1: Single anomalous service pair\n",
    "        if len(anomalypairs) == 1:\n",
    "            # Create topology feature structure\n",
    "            topoFea = {}\n",
    "            topoFea['time'] = t\n",
    "            topoFea['edges_info'] = []\n",
    "            \n",
    "            edge = anomalypairs[0]\n",
    "            topoedge = {}\n",
    "            topoedge['src'] = edge[0]  # Source service\n",
    "            topoedge['des'] = edge[1]  # Destination service\n",
    "            \n",
    "            # Extract 10-minute time window of metrics (t-9 to t)\n",
    "            topoedge['FailCount'] = ServicePairs[edge].loc[t-9:t]['FailCount'].tolist()\n",
    "            topoedge['Workload'] = ServicePairs[edge].loc[t-9:t]['Workload'].tolist()\n",
    "            topoedge['YesterFailCount'] = ServicePairs[edge].loc[t-9:t]['YesterFailCount'].tolist()\n",
    "            \n",
    "            topoFea['edges_info'].append(topoedge)\n",
    "            topoFea['MaxFail'] = topoedge['FailCount'][-1]  # Current failure count\n",
    "            topoFea['nodes'] = [edge[0], edge[1]]  # Services involved\n",
    "            topoFea['TimeStamp'] = ServicePairs[edge].loc[t]['TimeStamp']\n",
    "            \n",
    "            Topologies.append(topoFea)\n",
    "            \n",
    "        # Case 2: Multiple anomalous service pairs\n",
    "        elif len(anomalypairs) > 1:\n",
    "            # Extract failure count patterns for correlation analysis\n",
    "            point_list = [ServicePairs[pair].loc[t-9:t]['FailCount'].tolist() for pair in anomalypairs]\n",
    "            \n",
    "            # Calculate correlation matrix between anomaly patterns\n",
    "            distance_matrix = np.corrcoef(point_list)\n",
    "            distance_matrix[np.isnan(distance_matrix)] = 0  # Handle NaN values\n",
    "            \n",
    "            # Set diagonal to 1 (perfect self-correlation)\n",
    "            idx = [idx for idx in range(len(distance_matrix))]\n",
    "            distance_matrix[idx, idx] = 1\n",
    "            \n",
    "            # Convert correlation to distance for clustering\n",
    "            distance_matrix = np.abs(distance_matrix)\n",
    "            distance_matrix[distance_matrix >= THRESHOLD] = 1  # High correlation = close\n",
    "            distance_matrix[distance_matrix < THRESHOLD] = 2   # Low correlation = far\n",
    "            \n",
    "            # Use DBSCAN to cluster related anomalies\n",
    "            y_pred = DBSCAN(eps=1.5, min_samples=min_sample, metric='precomputed').fit_predict(distance_matrix).tolist()\n",
    "            \n",
    "            # Group anomalous pairs by cluster\n",
    "            clusters = [[] for i in range(max(y_pred)+1)]\n",
    "            for i, ano_pair in enumerate(anomalypairs):\n",
    "                clusters[y_pred[i]].append(ano_pair)\n",
    "            \n",
    "            # Process each cluster to extract connected topologies\n",
    "            for cluster in clusters:\n",
    "                # Create undirected and directed graphs\n",
    "                g = nx.Graph()        # For finding connected components\n",
    "                di_g = nx.DiGraph()   # For preserving edge directions\n",
    "                \n",
    "                g.add_edges_from(cluster)\n",
    "                di_g.add_edges_from(cluster)\n",
    "                \n",
    "                # Extract each connected component as a separate topology\n",
    "                for sub_g in nx.connected_components(g):\n",
    "                    topoFea = {}\n",
    "                    topoFea['time'] = t\n",
    "                    topoFea['edges_info'] = []\n",
    "                    MaxFail = 0\n",
    "                    \n",
    "                    # Extract features for each edge in this topology\n",
    "                    for edge in list(di_g.subgraph(sub_g).edges):\n",
    "                        topoedge = {}\n",
    "                        topoedge['src'] = edge[0]\n",
    "                        topoedge['des'] = edge[1]\n",
    "                        \n",
    "                        # Extract time series features\n",
    "                        topoedge['FailCount'] = ServicePairs[edge].loc[t-9:t]['FailCount'].tolist()\n",
    "                        topoedge['Workload'] = ServicePairs[edge].loc[t-9:t]['Workload'].tolist()\n",
    "                        topoedge['YesterFailCount'] = ServicePairs[edge].loc[t-9:t]['YesterFailCount'].tolist()\n",
    "                        \n",
    "                        topoFea['edges_info'].append(topoedge)\n",
    "                        MaxFail = max(MaxFail, topoedge['FailCount'][-1])\n",
    "                    \n",
    "                    topoFea['MaxFail'] = MaxFail\n",
    "                    topoFea['nodes'] = list(sub_g)\n",
    "                    topoFea['TimeStamp'] = ServicePairs[edge].loc[t]['TimeStamp']\n",
    "                    Topologies.append(topoFea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Verify Extraction Results\n",
    "\n",
    "Check the number of topologies extracted. This gives us an idea of how many potential incidents were identified in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the total number of extracted topologies\n",
    "# Each topology represents a potential incident impact structure\n",
    "len(Topologies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Raw Topologies\n",
    "\n",
    "Save the extracted topologies to a pickle file for use in subsequent analysis steps. This raw data will be processed further for labeling and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted topologies for further processing\n",
    "# These raw topologies will be used in the next steps for:\n",
    "# 1. Data labeling (identifying true incidents)\n",
    "# 2. Feature engineering (creating ML-ready features)\n",
    "# 3. Model training and evaluation\n",
    "with open('../data/raw_topoloies.pkl', 'wb') as f:\n",
    "    pickle.dump(Topologies, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
